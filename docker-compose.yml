services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
    volumes:
      - kafka-data:/var/lib/kafka/data
    restart: unless-stopped

  redis:
    image: redis:7
    container_name: redis
    command: ["redis-server","--appendonly","yes"]
    volumes:
      - redis-data:/data
    restart: unless-stopped

 
  kafka-init:
    build:
      context: ./kafka-components/kafka-init
      dockerfile: Dockerfile
    container_name: kafka-init
    depends_on:
      - kafka
    environment:
      KAFKA_BROKER: kafka:9092
    # opzionali:
    # DEFAULT_PARTITIONS: "3"
    # DEFAULT_RF: "1"
    # APPEND_RETENTION_MS: "604800000"         # 7 giorni
    # COMPACT_DELETE_RETENTION_MS: "86400000"  # 1 giorno
    restart: "no"
  
  kafka-connect:
    build:
      context: ./kafka-components/kafka-connect
      dockerfile: Dockerfile
    container_name: kafka-connect
    depends_on:
      - kafka
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:9092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_GROUP_ID: "connect-cluster-1"
      CONNECT_CONFIG_STORAGE_TOPIC: "_connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "_connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "_connect-status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
    ports:
      - "8083:8083"   # REST (utile per debug)
    restart: unless-stopped
  
  connect-init:
    build:
      context: ./kafka-components/kafka-connect/connect-init
      dockerfile: Dockerfile
    container_name: connect-init
    depends_on:
      - kafka-connect
      - postgres
      - kafka-init
    environment:
      CONNECT_URL: "http://kafka-connect:8083"
      KAFKA_BROKER: "kafka:9092"
      PG_HOST: "postgres"
      PG_PORT: "5432"
      PG_DB: "smart_shelf"
      PG_USER: "bdt_user"
      PG_PASS: "bdt_password"
    restart: "no"

  postgres:
    image: postgres:16
    container_name: postgres
    environment:
      POSTGRES_USER: bdt_user
      POSTGRES_PASSWORD: bdt_password
      POSTGRES_DB: smart_shelf
    ports:
      - "5432:5432"   
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgresql:/docker-entrypoint-initdb.d:ro
      - ./data/db_csv:/import/csv:ro
    restart: unless-stopped


  # Producers
  kafka-producer-foot-traffic:
    build:
      context: ./kafka-components/kafka-producer-foot_traffic
      dockerfile: Dockerfile
    container_name: kafka-producer-foot-traffic
    depends_on:
      - kafka
      - redis
    environment:
      KAFKA_BROKER: kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_STREAM: foot_traffic
    restart: unless-stopped
  
  kafka-producer-pos:
    build:
      context: ./kafka-components/kafka-producer-pos
      dockerfile: Dockerfile
    container_name: kafka-producer-pos
    depends_on:
      - kafka
      - redis
    environment:
      KAFKA_BROKER: kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_STREAM: pos_transactions
    volumes:
      - ./data:/data:ro
    restart: unless-stopped
  
  kafka-producer-shelf:
    build:
      context: ./kafka-components/kafka-producer-shelf
      dockerfile: Dockerfile
    container_name: kafka-producer-shelf
    depends_on:
      - kafka
      - redis
    environment:
      KAFKA_BROKER: kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_STREAM: shelf_events
    volumes:
      - ./data:/data:ro
    restart: unless-stopped

  kafka-producer-wh_shelf:
    build:
      context: ./kafka-components/kafka-producer-wh_shelf
      dockerfile: Dockerfile
    container_name: kafka-producer-wh_shelf
    depends_on:
      - kafka
      - redis
    environment:
      KAFKA_BROKER: kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: "6379"
      REDIS_DB: "0"
      REDIS_STREAM: wh_events
    volumes:
      - ./data:/data:ro
    restart: unless-stopped

  spark-init-delta:
    build:
      context: ./spark-apps/deltalake
      dockerfile: Dockerfile
    container_name: spark-init-delta
    environment:
      DELTA_ROOT: /delta
      SPARK_WAREHOUSE_DIR: /tmp/spark-warehouse
      PYSPARK_SUBMIT_ARGS: >
        --packages io.delta:delta-spark_2.12:3.2.0
        --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        pyspark-shell
    volumes:
      - ./delta:/delta
      - ./spark-apps/deltalake/init_delta.py:/app/init_delta.py:ro
    command: ["python","-u","/app/init_delta.py"]
    restart: "no"

volumes:
  kafka-data:
  redis-data:
  postgres-data:

