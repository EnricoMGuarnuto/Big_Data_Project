services:
# Kafka and Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    container_name: kafka
    depends_on:
    - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
    volumes:
    - kafka-data:/var/lib/kafka/data
    restart: unless-stopped

# Redis
  redis:
    image: redis:7
    container_name: redis
    command:
    - redis-server
    - --appendonly
    - 'yes'
    volumes:
    - redis-data:/data
    restart: unless-stopped

  sim-clock:
    build:
      context: .
      dockerfile: simulated_time/Dockerfile  
    container_name: sim-clock
    depends_on:
      - redis
    env_file:
      - .env
    volumes:
      - ./simulated_time:/app/simulated_time:ro
    environment:
      PYTHONPATH: /app
    restart: 'no'

# Kafka Connect and Init
  kafka-init:
    build:
      context: ./kafka-components/kafka-init
      dockerfile: Dockerfile
    container_name: kafka-init
    depends_on:
    - kafka
    environment:
      KAFKA_BROKER: kafka:9092
    restart: 'no'

  kafka-connect:
    build:
      context: ./kafka-components/kafka-connect
      dockerfile: Dockerfile
    container_name: kafka-connect
    depends_on:
    - kafka
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_GROUP_ID: connect-cluster-1
      CONNECT_CONFIG_STORAGE_TOPIC: _connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'false'
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
    ports:
    - 8083:8083
    restart: unless-stopped

  connect-init:
    build:
      context: ./kafka-components/kafka-connect/connect-init
      dockerfile: Dockerfile
    container_name: connect-init
    depends_on:
    - kafka-connect
    - postgres
    - kafka-init
    environment:
      CONNECT_URL: http://kafka-connect:8083
      KAFKA_BROKER: kafka:9092
      PG_HOST: postgres
      PG_PORT: '5432'
      PG_DB: smart_shelf
      PG_USER: bdt_user
      PG_PASS: bdt_password
      CONNECT_INIT_ENABLE_SOURCES: "1"
      CONNECT_INIT_ENABLE_SINKS: "1"
      # Default OFF to avoid double-writing: `alerts-sink` already writes to Postgres (WRITE_TO_PG=1)
      CONNECT_INIT_ENABLE_ALERTS_SINK: "0"
    restart: 'no'

# PostgreSQL Database
  postgres:
    image: postgres:16
    container_name: postgres
    environment:
      POSTGRES_USER: bdt_user
      POSTGRES_PASSWORD: bdt_password
      POSTGRES_DB: smart_shelf
    ports:
    - 5432:5432
    volumes:
    - postgres-data:/var/lib/postgresql/data
    - ./postgresql:/docker-entrypoint-initdb.d:ro
    - ./data/db_csv:/import/csv/db:ro
    - ./data/sim_out:/import/csv/sim:ro
    restart: unless-stopped

# Kafka Producers
  kafka-producer-foot-traffic:
    build:
      context: ./kafka-components/kafka-producer-foot_traffic
      dockerfile: Dockerfile
    container_name: kafka-producer-foot-traffic
    depends_on:
    - kafka
    - redis
    env_file:
    - .env
    environment:
      KAFKA_BROKER: kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: '6379'
      REDIS_DB: '0'
      REDIS_STREAM: foot_traffic
      SLEEP: '0.125'
      TIME_SCALE: '4.0'
      PYTHONPATH: /app
    volumes:
    - ./data:/data:ro
    - ./simulated_time:/app/simulated_time:ro  
    restart: unless-stopped

  kafka-producer-pos:
    build:
      context: ./kafka-components/kafka-producer-pos
      dockerfile: Dockerfile
    container_name: kafka-producer-pos
    depends_on:
    - kafka
    - redis
    env_file:
    - .env
    environment:
      KAFKA_BROKER: kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: '6379'
      REDIS_DB: '0'
      REDIS_STREAM: pos_transactions
      SLEEP: '0.25'
      TIME_SCALE: '4.0'
      PYTHONPATH: /app   # âœ… importa i moduli clock + config
    volumes:
    - ./data:/data:ro
    - ./simulated_time:/app/simulated_time:ro
    restart: unless-stopped

  kafka-producer-shelf:
    build:
      context: ./kafka-components/kafka-producer-shelf
      dockerfile: Dockerfile
    container_name: kafka-producer-shelf
    depends_on:
    - kafka
    - redis
    env_file:
    - .env
    environment:
      KAFKA_BROKER: kafka:9092
      REDIS_HOST: redis
      REDIS_PORT: '6379'
      REDIS_DB: '0'
      REDIS_STREAM: shelf_events
      SHELF_SLEEP: '0.25'
      PYTHONPATH: /app
    volumes:
    - ./data:/data:ro
    - ./simulated_time:/app/simulated_time:ro
    restart: unless-stopped
  
#Spark Applications
  spark-init-delta:
    build:
      context: ./spark-apps/deltalake
      dockerfile: Dockerfile
    container_name: spark-init-delta
    environment:
      DELTA_ROOT: /delta
      SPARK_WAREHOUSE_DIR: /tmp/spark-warehouse
      PYSPARK_SUBMIT_ARGS: '--packages io.delta:delta-spark_2.12:3.2.0 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        pyspark-shell'
    volumes:
    - ./delta:/delta
    - ./spark-apps/deltalake/init_delta.py:/app/init_delta.py:ro
    command:
    - python
    - -u
    - /app/init_delta.py
    restart: 'no'

  spark-shelf-aggregator:
    build:
      context: ./spark-apps/shelf-aggregator
      dockerfile: Dockerfile
    container_name: spark-shelf-aggregator
    depends_on:
    - kafka
    - redis
    env_file:
    - .env
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_SHELF_EVENTS: shelf_events
      TOPIC_SHELF_PROFILES: shelf_profiles
      TOPIC_SHELF_STATE: shelf_state
      JDBC_PG_URL: jdbc:postgresql://postgres:5432/smart_shelf
      JDBC_PG_USER: bdt_user
      JDBC_PG_PASSWORD: bdt_password
      BOOTSTRAP_FROM_PG: '1'
      DELTA_ROOT: /delta
      STARTING_OFFSETS: earliest
      MAX_OFFSETS_PER_TRIGGER: "2000"
      CHECKPOINT_ROOT: /delta/_checkpoints/shelf_aggregator
      PYTHONPATH: /app
    volumes:
    - ./delta:/delta
    - ./simulated_time:/app/simulated_time:ro
    restart: 'no'

  batch-state-updater:
    build:
      context: ./spark-apps/batch-state-updater
    env_file:
    - .env
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_POS_TRANSACTIONS: pos_transactions
      TOPIC_BATCH_STATE: shelf_batch_state
      JDBC_PG_URL: jdbc:postgresql://postgres:5432/smart_shelf
      JDBC_PG_USER: bdt_user
      JDBC_PG_PASSWORD: bdt_password
      BOOTSTRAP_FROM_PG: '1'
      DELTA_ROOT: /delta
      CHECKPOINT_ROOT: /delta/_checkpoints/batch_state_updater
      STARTING_OFFSETS: earliest
      MAX_OFFSETS_PER_TRIGGER: "2000"
      PYTHONPATH: /app
    volumes:
    - ./delta:/delta
    - ./simulated_time:/app/simulated_time:ro
    depends_on:
    - kafka
    - postgres
    - redis
    restart: 'no'

  foot-traffic-raw-sink:
    build:
      context: ./spark-apps/foot-traffic-raw-sink
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_FOOT_TRAFFIC: foot_traffic
      STARTING_OFFSETS: earliest
      DELTA_ROOT: /delta
      DL_FOOT_TRAFFIC_PATH: /delta/raw/foot_traffic
      CHECKPOINT: /delta/_checkpoints/foot_traffic_raw_sink
    volumes:
    - ./delta:/delta
    depends_on:
    - kafka
    restart: 'no'

  shelf-alert-engine:
    build:
      context: ./spark-apps/shelf-alert-engine
    env_file:
      - .env 
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_SHELF_STATE: shelf_state
      TOPIC_SHELF_BATCH_STATE: shelf_batch_state
      TOPIC_SHELF_POLICIES: shelf_policies
      TOPIC_ALERTS: alerts
      TOPIC_SHELF_RESTOCK: shelf_restock_plan
      DELTA_ROOT: /delta
      DL_SHELF_STATE_PATH: /delta/cleansed/shelf_state
      DL_SHELF_BATCH_PATH: /delta/cleansed/shelf_batch_state
      DL_ALERTS_PATH: /delta/ops/alerts
      DL_RESTOCK_PATH: /delta/ops/shelf_restock_plan
      CHECKPOINT_ROOT: /delta/_checkpoints/shelf_alert_engine
      STARTING_OFFSETS: earliest
      NEAR_EXPIRY_DAYS: "5"
      DEFAULT_TARGET_PCT: "80.0"
      LOAD_MAX_FROM_PG: "1"
      JDBC_PG_URL: jdbc:postgresql://postgres:5432/smart_shelf
      JDBC_PG_USER: bdt_user
      JDBC_PG_PASSWORD: bdt_password
      MAX_STOCK_TABLE: ref.store_inventory_snapshot
      BOOTSTRAP_POLICIES_FROM_PG: "1"
      POLICIES_PG_TABLE: config.shelf_policies
      PYTHONPATH: /app 
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - kafka
      - postgres
      - redis
    restart: 'no'

  wh-alert-engine:
    build:
      context: ./spark-apps/wh-alert-engine
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_WH_STATE: wh_state
      TOPIC_WH_BATCH_STATE: wh_batch_state
      TOPIC_WH_POLICIES: wh_policies
      TOPIC_ALERTS: alerts
      TOPIC_WH_SUPPLIER_PLAN: wh_supplier_plan
      DELTA_ROOT: /delta
      DL_WH_STATE_PATH: /delta/cleansed/wh_state
      DL_WH_BATCH_PATH: /delta/cleansed/wh_batch_state
      DL_ALERTS_PATH: /delta/ops/alerts
      DL_SUPPLIER_PLAN_PATH: /delta/ops/wh_supplier_plan
      CHECKPOINT_ROOT: /delta/_checkpoints/wh_alert_engine
      STARTING_OFFSETS: earliest
      NEAR_EXPIRY_DAYS: "10"
      DEFAULT_MULTIPLIER: "1"
      BOOTSTRAP_WH_POLICIES_FROM_PG: "1"
      JDBC_PG_URL: jdbc:postgresql://postgres:5432/smart_shelf
      JDBC_PG_USER: bdt_user
      JDBC_PG_PASSWORD: bdt_password
      WH_POLICIES_PG_TABLE: config.wh_policies
      PYTHONPATH: /app
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - kafka
      - postgres
      - redis
    restart: 'no'

  alerts-sink:
    build:
      context: ./spark-apps/alerts-sink
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_ALERTS: alerts
      STARTING_OFFSETS: earliest

      DELTA_ROOT: /delta
      DL_ALERTS_PATH: /delta/ops/alerts
      CHECKPOINT: /delta/_checkpoints/alerts_sink
      # Postgres sink (optional)
      WRITE_TO_PG: "1"
      JDBC_PG_URL: jdbc:postgresql://postgres:5432/smart_shelf
      JDBC_PG_USER: bdt_user
      JDBC_PG_PASSWORD: bdt_password
      PG_TABLE: ops.alerts
    volumes:
      - ./delta:/delta
    depends_on:
      - kafka
      - postgres
    restart: 'no'

  shelf-restock-manager:
    build:
      context: ./spark-apps/shelf-restock-manager
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_RESTOCK: shelf_restock_plan
      TOPIC_WH_EVENTS: wh_events
      TOPIC_ALERTS: alerts
      DELTA_ROOT: /delta
      DL_WH_BATCH_PATH: /delta/cleansed/wh_batch_state
      DL_RESTOCK_PATH: /delta/ops/shelf_restock_plan
      CKP_ROOT: /delta/_checkpoints/shelf_restock_manager
      STARTING_OFFSETS: earliest
      PLAN_DELAY_SEC: "60" # attesa di 1 minuto prima di muovere stock
      PYTHONPATH: /app    
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - kafka
      - redis
    env_file:
     - .env
    restart: 'no'

  wh-aggregator:
    build:
      context: ./spark-apps/wh-aggregator
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_WH_EVENTS: wh_events
      TOPIC_WH_STATE: wh_state
      DELTA_ROOT: /delta
      STARTING_OFFSETS: earliest
      BOOTSTRAP_FROM_PG: "1"
      JDBC_PG_URL: jdbc:postgresql://postgres:5432/smart_shelf
      JDBC_PG_USER: bdt_user
      JDBC_PG_PASSWORD: bdt_password
      PYTHONPATH: /app
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - kafka
      - postgres
    env_file:
    - .env
    restart: 'no'

  wh-batch-state-updater:
    build:
      context: ./spark-apps/wh-batch-state-updater
    env_file:
     - .env
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_WH_EVENTS: wh_events
      TOPIC_WH_BATCH: wh_batch_state
      TOPIC_SHELF_BATCH: shelf_batch_state
      DELTA_ROOT: /delta
      STARTING_OFFSETS: earliest
      BOOTSTRAP_FROM_PG: "1"
      JDBC_PG_URL: jdbc:postgresql://postgres:5432/smart_shelf
      JDBC_PG_USER: bdt_user
      JDBC_PG_PASSWORD: bdt_password
      PYTHONPATH: /app
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - kafka
      - postgres
      - redis
    restart: 'no'

  shelf-refill-bridge:
    build:
      context: ./spark-apps/shelf-refill-bridge
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_WH_EVENTS: wh_events
      TOPIC_SHELF_EVENTS: shelf_events
      TOPIC_SHELF_PROFILES: shelf_profiles
      DELAY_MINUTES: "3"
      DELTA_ROOT: /delta
      PENDING_PATH: /delta/staging/shelf_refill_pending
      CKP: /delta/_checkpoints/shelf_refill_bridge
      STARTING_OFFSETS: earliest
      PYTHONPATH: /app 
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    env_file:
     - .env
    depends_on:
      - kafka
      - redis
    restart: 'no'


  daily-discount-manager:
    build:
      context: ./spark-apps/daily-discount-manager
      dockerfile: Dockerfile
    container_name: daily-discount-manager
    env_file:
    - .env 
    depends_on:
      - kafka
      - redis
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_DAILY_DISCOUNTS: daily_discounts
      TOPIC_ALERTS: alerts

      DELTA_ROOT: /delta
      DL_SHELF_BATCH_PATH: /delta/cleansed/shelf_batch_state
      DL_DAILY_DISC_PATH: /delta/analytics/daily_discounts
      PYTHONPATH: /app
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    restart: 'unless-stopped'

  removal-scheduler:
    build:
      context: ./spark-apps/removal-scheduler
      dockerfile: Dockerfile
    container_name: removal-scheduler
    depends_on:
      - kafka
      - redis
    env_file:
     - .env
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_ALERTS: alerts
      TOPIC_SHELF_STATE: shelf_state
      TOPIC_SHELF_BATCH_STATE: shelf_batch_state

      DELTA_ROOT: /delta
      DL_SHELF_STATE_PATH: /delta/cleansed/shelf_state
      DL_SHELF_BATCH_PATH: /delta/cleansed/shelf_batch_state

      PYTHONPATH: /app

      # remove next day by default
      REMOVE_MODE: next_day
      EMIT_ALERTS: "1"
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    restart: 'no'


  wh-supplier-manager:
    build:
      context: ./spark-apps/wh-supplier-manager
      dockerfile: Dockerfile
    container_name: wh-supplier-manager
    env_file:
    - .env
    environment:
      KAFKA_BROKER: kafka:9092
      TOPIC_WH_SUPPLIER_PLAN: wh_supplier_plan
      TOPIC_WH_EVENTS: wh_events

      DELTA_ROOT: /delta
      DL_SUPPLIER_PLAN_PATH: /delta/ops/wh_supplier_plan
      DL_ORDERS_PATH: /delta/ops/wh_supplier_orders
      DL_RECEIPTS_PATH: /delta/ops/wh_supplier_receipts
      DL_WH_EVENTS_RAW: /delta/raw/wh_events

      CHECKPOINT_ROOT: /delta/_checkpoints/wh_supplier_manager
      TICK_MINUTES: "1"
      PYTHONPATH: /app

      # Schedule (UTC)
      CUTOFF_HOUR: "12"
      CUTOFF_MINUTE: "0"
      DELIVERY_HOUR: "8"
      DELIVERY_MINUTE: "0"

      DEFAULT_EXPIRY_DAYS: "365"
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - kafka
      - spark-init-delta
      - wh-alert-engine
      - redis
    restart: 'no'

  shelf-daily-features:
    build:
      context: ./spark-apps/shelf-daily-features
    container_name: shelf-daily-features
    environment:
      PGHOST: postgres
      PGPORT: "5432"
      PGDATABASE: smart_shelf
      PGUSER: bdt_user
      PGPASSWORD: bdt_password
      PYTHONPATH: /app
    volumes:
      - ./delta:/delta
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - kafka
      - spark-init-delta
      - wh-alert-engine
      - redis
    restart: "no"


# ml model
  training-service:
    build:
      context: ./ml-model/training-service
    environment:
      PG_DSN: postgresql+psycopg2://bdt_user:bdt_password@postgres:5432/smart_shelf
      MODEL_NAME: xgb_batches_to_order
      ARTIFACT_DIR: /models
      RETRAIN_DAYS: 7
      PYTHONPATH: /app                
    volumes:
      - ./models:/models
      - ./simulated_time:/app/simulated_time:ro
    depends_on:
      - postgres
      - redis
      - sim-clock
    restart: unless-stopped


  inference-service:
    build:
      context: ./ml-model/inference-service
    environment:
      PG_DSN: postgresql+psycopg2://bdt_user:bdt_password@postgres:5432/smart_shelf
      MODEL_NAME: xgb_batches_to_order
      ARTIFACT_DIR: /models
      RUN_HOUR: 00
      RUN_MINUTE: 05
      PYTHONPATH: /app
      PYTHONUNBUFFERED: "1"
      HEARTBEAT_SECONDS: "10"
    volumes:
      - ./models:/models
      - ./simulated_time:/app/simulated_time:ro   
      - ./delta:/delta
    depends_on:
      - postgres
      - redis
      - sim-clock
      - training-service
    restart: unless-stopped



# Dashboard Service
  dashboard:
    build:
      context: .
      dockerfile: dashboard/Dockerfile
    container_name: dashboard
    ports:
      - "8501:8501"
    environment:
      INVENTORY_CSV: "data/db_csv/store_inventory_final.csv"
      LAYOUT_YAML: "dashboard/store_layout.yaml"
      DELTA_EVENTS_PATH: "delta/raw/weight_events"
      DELTA_TIMESTAMP_COL: "event_ts"
      POSTGRES_HOST: "postgres"
      POSTGRES_PORT: "5432"
      POSTGRES_DB: "smart_shelf"
      POSTGRES_USER: "bdt_user"
      POSTGRES_PASSWORD: "bdt_password"
      ALERTS_TABLE: "ops.alerts"
      DASH_REFRESH_MS: "1000"
      BLINK_SECONDS: "1.2"
      PYTHONPATH: /app
    depends_on:
      - postgres
    volumes:
      - ./data:/app/data
      - ./dashboard:/app/dashboard
      - ./images:/app/images
      - ./delta:/app/delta
      - ./simulated_time:/app/simulated_time:ro



volumes:
  kafka-data: null
  redis-data: null
  postgres-data: null
